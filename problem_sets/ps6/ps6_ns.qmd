---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Peter Ganong, Maggie Shi, and Andre Oviedo"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." # NS #
2. "I have uploaded the names of anyone I worked with on the problem set ** # N/A#
3. Late coins used this pset: 0

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
import pandas as pd
import zipfile

# unzipping data
waze_path = "/Users/neilstein/Documents/Academic/Fall 24/Python II/student30538-2/problem_sets/ps6/waze_data.zip"
with zipfile.ZipFile(waze_path, "r") as zip_ref:
  with zip_ref.open("waze_data_sample.csv", "r") as file:
    waze_df = pd.read_csv(file)

# variable names and data types:
for col in waze_df.columns[:-3]:
  print(f"Column: {col}, Data Type: {waze_df[col].dtype}")
# copied printed results to markdown table below
```

# Markdown table for data typology

| Variable Name | Altair Data Type| 
| --- | --- |
| Unnamed: 0 |  Nominal |
| city | Nominal |
| confidence | Ordinal |
| nThumbsUp | Quantitative |
| street | Nominal |
| uuid | Nominal |
| country | Nominal |
| type | Nominal |
| subtype | Nominal |
| roadType | Nominal or Ordinal |
| reliability | Ordinal |
| magvar | Nominal |
| reportRating | Nominal or Ordinal |

2. 

```{python}
import pandas as pd
import altair as alt
import zipfile

# unzipping the full dataset
waze_path = "/Users/neilstein/Documents/Academic/Fall 24/Python II/student30538-2/problem_sets/ps6/waze_data.zip"
with zipfile.ZipFile(waze_path, "r") as zip_ref:
  with zip_ref.open("waze_data.csv", "r") as file:
    full_waze_df = pd.read_csv(file)

# counting of NA vs. present values
counts_df = full_waze_df.isnull().sum().to_frame("NA")
counts_df["Present"] = full_waze_df.shape[0] - counts_df["NA"]
counts_df = counts_df.reset_index().rename(columns={"index": "Column_Name"})

# Melt the DataFrame for Altair
waze_df_melted = counts_df.melt(id_vars= "Column_Name", var_name= "Value_Type", value_name= "Count")

# Create the stacked bar chart using Altair
waze_NA_chart = alt.Chart(waze_df_melted).mark_bar().encode(
    x= "Column_Name",
    y= "Count",
    color= "Value_Type"
).properties(
    title= "NA and non-NA Value Counts per Column"
)
waze_NA_chart.show()
```

Interpretation -- we can observe from the chart that nThumbsUp is almost entirely missing (99% null value), whereas the other columns that are missing values, street and subtype, are only missing at ~2% and ~12% of the total values. 

3. 

```{python}
import pandas as pd

# finding unique value for 'Type' and 'Subtype'
type_unique = full_waze_df["type"].unique().tolist()
subtype_unique = full_waze_df["subtype"].unique().tolist()
print(type_unique)
print(subtype_unique)

# counting combinations
waze_combo_df = full_waze_df.copy()
waze_combo_df.fillna("Unclassified", inplace= True) # without this, NA is dropped

type_grouping = waze_combo_df.groupby(["type", "subtype"]).size().reset_index(name= "count")
print(type_grouping)
```

```{markdown}
# Question 3 detailed responses
1) There are 4 types that have a subtype NA ("Unclassified" in my code)

2) There are several columns that we can reasonable see containing sub-subtypes, these include:
- HAZARD_ON_ROAD: this is a subtype that contains several different hazards occuring on the road, including:
  - EMERGENCY_VEHICLE
  - ICE
  - LANE_CLOSED
  - OBJECT
  - POT_HOLE
  - ROAD_KILL
  - TRAFFIC_LIGHT
- HAZARD_ON_SHOULDER: this is a subtype containing different traffic issues, such as:
  - ANIMALS
  - CAR_STOPPED
  - MISSING_SIGN
- HAZARD_WEATHER: there is a general category, as well as specific sub-subtypes
  - FLOOD
  - FOG
  - HAIL
  - HEAVY_SNOW
Looking at the remaining fields, there does not appear to be a sub-subtype as they are all specific variations of a subtype.

3) The missing or "NA" subtypes appear to be a relatively small portion of the HAZARD type, whereas they are a far bigger proportion of the ROAD_CLOSE and JAM type, so my intuition here is that for the Hazard type there is enough specificity where the driver was able to quickly select a menu item, whereas it can be much harder for a driver to objectively rank the severity of a traffic jam or to know exactly the reasoning behind a road closure, so they would more likely choose a generic option. Hazards, especially of the types listed here, should be much easier to diagnose quickly!


I will be keeping the missing values in my data and have already marked them as unclassified in a prior step (3)
```


4. 

```{python}
import pandas as pd

# crosswalk creation (a) -- ignoring instructions, they are moot
crosswalk_df = type_grouping.copy()
crosswalk_df["updated_type"] = ""
crosswalk_df["updated_subtype"] = ""
crosswalk_df["updated_subsubtype"] = ""

# filling the crosswalk (b)
crosswalk_df["updated_type"] = crosswalk_df["type"]


for index, row in crosswalk_df.iterrows():
    subtype = row["subtype"]
    
    if "HAZARD_ON_ROAD_" in subtype:
        split_index = subtype.index("HAZARD_ON_ROAD_")
        crosswalk_df.at[index, "updated_subtype"] = subtype[:split_index]
        crosswalk_df.at[index, "updated_subsubtype"] = subtype[split_index+len("HAZARD_ON_ROAD_"):]
    elif "HAZARD_ON_SHOULDER_" in subtype:
        split_index = subtype.index("HAZARD_ON_SHOULDER_")
        crosswalk_df.at[index, "updated_subtype"] = subtype[:split_index]
        crosswalk_df.at[index, "updated_subsubtype"] = subtype[split_index+len("HAZARD_ON_SHOULDER_"):]
    elif "HAZARD_WEATHER_" in subtype:
        split_index = subtype.index("HAZARD_WEATHER_")
        crosswalk_df.at[index, "updated_subtype"] = subtype[:split_index]
        crosswalk_df.at[index, "updated_subsubtype"] = subtype[split_index+len("HAZARD_WEATHER_"):]
    else:
        crosswalk_df.at[index, "updated_subtype"] = subtype
        crosswalk_df.at[index, "updated_subsubtype"] = ""

    # initial results ended up with empty columns, adding a corrective loop
    if not row["updated_subtype"]:
        if "HAZARD_ON_ROAD_" in row["subtype"]:
            crosswalk_df.at[index, "updated_subtype"] = "HAZARD_ON_ROAD"
        elif "HAZARD_ON_SHOULDER_" in row["subtype"]:
            crosswalk_df.at[index, "updated_subtype"] = "HAZARD_ON_SHOULDER"
        elif "HAZARD_WEATHER_" in row["subtype"]:
            crosswalk_df.at[index, "updated_subtype"] = "HAZARD_WEATHER"

# merging the crosswalk (c) -- original instructions don't make logical sense, switching to a full df and not a 32 row version
waze_merged_df = full_waze_df.copy()

mapping_dict = crosswalk_df.set_index(["type", "subtype"]).to_dict(orient= "index")

for index, row in waze_merged_df.iterrows():
    key = (row["type"], row["subtype"])
    if key in mapping_dict:
        waze_merged_df.at[index, "updated_type"] = mapping_dict[key]["updated_type"]
        waze_merged_df.at[index, "updated_subtype"] = mapping_dict[key]["updated_subtype"]

# counting the accident + na values
for index, row in type_grouping.iterrows():
    if row["type"] == "ACCIDENT" and row["subtype"] == "Unclassified":
        print(f"The count for ACCIDENT and Unclassified is: {row['count']}")
```

# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
import pandas as pd
import re

# per the prompt, I asked Chat GPT "how can I use regular expressions to extract the latitude and longitude into two new columns of data from a dataset where it is currently stored like this: "POINT(-87.676685 41.929692)""

# regular expression function creation
def extract_coordinates(geo_str):
    match = re.search(r"\((-?\d+\.\d+)\s(-?\d+\.\d+)\)", geo_str)
    if match:
        return match.group(1), match.group(2)
    else:
        return None, None

# applying the function to the full dataframe
waze_merged_df[["latitude", "longitude"]] = waze_merged_df["geo"].apply(extract_coordinates).apply(pd.Series)
```

b. 
```{python}
import pandas as pd
import numpy as np

# binning the latitude and longitudes
waze_merged_df["latitude"] = pd.to_numeric(waze_merged_df["latitude"], errors= "coerce")
waze_merged_df["longitude"] = pd.to_numeric(waze_merged_df["longitude"], errors= "coerce")


bins = np.arange(-180, 181, 0.01)
waze_merged_df["lat_bin"] = pd.cut(waze_merged_df["latitude"], bins= bins)
waze_merged_df["long_bin"] = pd.cut(waze_merged_df["longitude"], bins= bins)


# chekcing for the most frequently observed lat-long combo
lat_long_combo = waze_merged_df.groupby(["lat_bin", "long_bin"]).size().reset_index(name= "count")
most_freq= lat_long_combo.loc[lat_long_combo["count"].idxmax()]

print(f"Most frequent lat-long combo: Latitude={most_freq['lat_bin']}, Longitude={most_freq['long_bin']}, with {most_freq['count']} observations")

```


c. 
```{python}
import pandas as pd
import numpy as np

# aggregation to the top 10 lat-long bins -- choosing HAZARD_ON_SHOULDER_CAR_STOPPED as my preferred chosen alert as it has the most samples
hazard_waze = waze_merged_df[waze_merged_df["subtype"] == "HAZARD_ON_SHOULDER_CAR_STOPPED"]

# checking tigheter bins (fewer decimal points)
new_bins = np.arange(-180, 181, 0.1)
hazard_waze["lat_bin"] = pd.cut(hazard_waze["latitude"], bins= new_bins)
hazard_waze["long_bin"] = pd.cut(hazard_waze["longitude"], bins= new_bins)

hazard_grouping_df = hazard_waze.groupby(["lat_bin", "long_bin"]).size().reset_index(name= "count")
hazard_top_10 = hazard_grouping_df.sort_values(by= "count", ascending=False).head(10)
```

This approach, while somewhat understandable given the length of the dataframe, fails the logic test of being useful. These bins are accurately holding data, aggregated to the top ten categories, however the useful-ness of this data is next to zero as in order to do so we had to blur the geospatial precision, leading to dots that will be too large to be meaningful. 



d. 
```{python}

```

3. 
    
a. 

```{python}

```
    

b. 
```{python}
'''
# MODIFY ACCORDINGLY
file_path = "./top_alerts_map/chicago-boundaries.geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])
'''
```

4. 

```{python}

```

5. 

a. 

```{python}
from shiny import App, render, ui

```

b. 
```{python}

```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
